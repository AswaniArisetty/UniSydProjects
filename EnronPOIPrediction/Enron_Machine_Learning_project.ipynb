{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud at Enron\n",
    "\n",
    "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. Before  2001, Enron employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications and pulp and paper companies, with claimed revenues of nearly $111 billion during 2000. \n",
    "\n",
    "But the Enron scandal , comprising unethical practices and exploiting accounting limitations to misrepresent earnings and modify the balance sheet to indicate favorable performance, revealed in October 2001, led to the bankruptcy of the Enron Corporation and the majority of them were perpetuated by the indirect knowledge or direct actions of CFO , CEO, and a few other executives.\n",
    "\n",
    "#### Enron corpus :\n",
    "Enron Corpus is a large database of over 600,000 emails generated by 158 employees of the Enron Corporation and acquired by the Federal Energy Regulatory Commission during its investigation after the company's collapse.\n",
    "In this project , we are analysing the latest available Enron dataset (courtesy : Udacity and https://www.cs.cmu.edu/~./enron/ ) to identify the POI (People of Interest) from around 150 former employees of Enron corporation.\n",
    "\n",
    "### Project Goal : \n",
    "The aim of this project is to design a supervised machine learning model that would be able to classify if an employee is a POI or non-POI , using the Financial and Email features provided in the enron corpus as inputs. \n",
    "\n",
    "### Data Analysis :\n",
    "The data from final_project_dataset pickle file has been loaded into a Pandas DataFrame (enron_df) for easy analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'salary', u'to_messages', u'deferral_payments', u'total_payments',\n",
      "       u'exercised_stock_options', u'bonus', u'restricted_stock',\n",
      "       u'shared_receipt_with_poi', u'restricted_stock_deferred',\n",
      "       u'total_stock_value', u'expenses', u'loan_advances', u'from_messages',\n",
      "       u'other', u'from_this_person_to_poi', u'poi', u'director_fees',\n",
      "       u'deferred_income', u'long_term_incentive', u'email_address',\n",
      "       u'from_poi_to_this_person'],\n",
      "      dtype='object') \n",
      "\n",
      " Total rowsXcolumns (146, 21)\n"
     ]
    }
   ],
   "source": [
    "# About the data\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append(\"tools/\")\n",
    "\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "enron_df=pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "print enron_df.columns, '\\n' ,'\\n Total rowsXcolumns', enron_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of POI= 18\n",
      "Total number of non-POI= 128\n"
     ]
    }
   ],
   "source": [
    "print 'Total number of POI=',len(enron_df[enron_df.poi==True])\n",
    "print 'Total number of non-POI=',len(enron_df[enron_df.poi==False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this is an unbalanced data set (15-85 ratio). As there are financial attributes , let us analyse top 10 employees that were highly paid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>...</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>poi</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>email_address</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>26704229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32083396.0</td>\n",
       "      <td>309886585.0</td>\n",
       "      <td>311764000.0</td>\n",
       "      <td>97343619.0</td>\n",
       "      <td>130322299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7576788.0</td>\n",
       "      <td>434509511.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83925000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42667589.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1398517.0</td>\n",
       "      <td>-27992891.0</td>\n",
       "      <td>48521928.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAY KENNETH L</th>\n",
       "      <td>1072321.0</td>\n",
       "      <td>4273.0</td>\n",
       "      <td>202911.0</td>\n",
       "      <td>103559793.0</td>\n",
       "      <td>34348384.0</td>\n",
       "      <td>7000000.0</td>\n",
       "      <td>14761694.0</td>\n",
       "      <td>2411.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49110078.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81525000.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10359729.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-300000.0</td>\n",
       "      <td>3600000.0</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREVERT MARK A</th>\n",
       "      <td>1060932.0</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>6426990.0</td>\n",
       "      <td>17252530.0</td>\n",
       "      <td>10433518.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>4188667.0</td>\n",
       "      <td>2979.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14622185.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7427621.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3367011.0</td>\n",
       "      <td>1617011.0</td>\n",
       "      <td>mark.frevert@enron.com</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>NaN</td>\n",
       "      <td>523.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanjay.bhatnagar@enron.com</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAVORATO JOHN J</th>\n",
       "      <td>339288.0</td>\n",
       "      <td>7259.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10425757.0</td>\n",
       "      <td>4158995.0</td>\n",
       "      <td>8000000.0</td>\n",
       "      <td>1008149.0</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5167144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2585.0</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2035380.0</td>\n",
       "      <td>john.lavorato@enron.com</td>\n",
       "      <td>528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKILLING JEFFREY K</th>\n",
       "      <td>1111258.0</td>\n",
       "      <td>3627.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8682716.0</td>\n",
       "      <td>19250000.0</td>\n",
       "      <td>5600000.0</td>\n",
       "      <td>6843672.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26093672.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>22122.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1920000.0</td>\n",
       "      <td>jeff.skilling@enron.com</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARTIN AMANDA K</th>\n",
       "      <td>349487.0</td>\n",
       "      <td>1522.0</td>\n",
       "      <td>85430.0</td>\n",
       "      <td>8407016.0</td>\n",
       "      <td>2070306.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>477.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2070306.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>230.0</td>\n",
       "      <td>2818454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5145434.0</td>\n",
       "      <td>a..martin@enron.com</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>267102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1295738.0</td>\n",
       "      <td>5634343.0</td>\n",
       "      <td>6680544.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>3942714.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10623258.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2660303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1386055.0</td>\n",
       "      <td>1586055.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BELDEN TIMOTHY N</th>\n",
       "      <td>213999.0</td>\n",
       "      <td>7991.0</td>\n",
       "      <td>2144013.0</td>\n",
       "      <td>5501630.0</td>\n",
       "      <td>953136.0</td>\n",
       "      <td>5249999.0</td>\n",
       "      <td>157569.0</td>\n",
       "      <td>5521.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1110705.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>484.0</td>\n",
       "      <td>210698.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2334434.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELAINEY DAVID W</th>\n",
       "      <td>365163.0</td>\n",
       "      <td>3093.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4747979.0</td>\n",
       "      <td>2291113.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>1323148.0</td>\n",
       "      <td>2097.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3614261.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3069.0</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1294981.0</td>\n",
       "      <td>david.delainey@enron.com</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        salary  to_messages  deferral_payments  \\\n",
       "TOTAL               26704229.0          NaN         32083396.0   \n",
       "LAY KENNETH L        1072321.0       4273.0           202911.0   \n",
       "FREVERT MARK A       1060932.0       3275.0          6426990.0   \n",
       "BHATNAGAR SANJAY           NaN        523.0                NaN   \n",
       "LAVORATO JOHN J       339288.0       7259.0                NaN   \n",
       "SKILLING JEFFREY K   1111258.0       3627.0                NaN   \n",
       "MARTIN AMANDA K       349487.0       1522.0            85430.0   \n",
       "BAXTER JOHN C         267102.0          NaN          1295738.0   \n",
       "BELDEN TIMOTHY N      213999.0       7991.0          2144013.0   \n",
       "DELAINEY DAVID W      365163.0       3093.0                NaN   \n",
       "\n",
       "                    total_payments  exercised_stock_options       bonus  \\\n",
       "TOTAL                  309886585.0              311764000.0  97343619.0   \n",
       "LAY KENNETH L          103559793.0               34348384.0   7000000.0   \n",
       "FREVERT MARK A          17252530.0               10433518.0   2000000.0   \n",
       "BHATNAGAR SANJAY        15456290.0                2604490.0         NaN   \n",
       "LAVORATO JOHN J         10425757.0                4158995.0   8000000.0   \n",
       "SKILLING JEFFREY K       8682716.0               19250000.0   5600000.0   \n",
       "MARTIN AMANDA K          8407016.0                2070306.0         NaN   \n",
       "BAXTER JOHN C            5634343.0                6680544.0   1200000.0   \n",
       "BELDEN TIMOTHY N         5501630.0                 953136.0   5249999.0   \n",
       "DELAINEY DAVID W         4747979.0                2291113.0   3000000.0   \n",
       "\n",
       "                    restricted_stock  shared_receipt_with_poi  \\\n",
       "TOTAL                    130322299.0                      NaN   \n",
       "LAY KENNETH L             14761694.0                   2411.0   \n",
       "FREVERT MARK A             4188667.0                   2979.0   \n",
       "BHATNAGAR SANJAY          -2604490.0                    463.0   \n",
       "LAVORATO JOHN J            1008149.0                   3962.0   \n",
       "SKILLING JEFFREY K         6843672.0                   2042.0   \n",
       "MARTIN AMANDA K                  NaN                    477.0   \n",
       "BAXTER JOHN C              3942714.0                      NaN   \n",
       "BELDEN TIMOTHY N            157569.0                   5521.0   \n",
       "DELAINEY DAVID W           1323148.0                   2097.0   \n",
       "\n",
       "                    restricted_stock_deferred  total_stock_value  \\\n",
       "TOTAL                              -7576788.0        434509511.0   \n",
       "LAY KENNETH L                             NaN         49110078.0   \n",
       "FREVERT MARK A                            NaN         14622185.0   \n",
       "BHATNAGAR SANJAY                   15456290.0                NaN   \n",
       "LAVORATO JOHN J                           NaN          5167144.0   \n",
       "SKILLING JEFFREY K                        NaN         26093672.0   \n",
       "MARTIN AMANDA K                           NaN          2070306.0   \n",
       "BAXTER JOHN C                             NaN         10623258.0   \n",
       "BELDEN TIMOTHY N                          NaN          1110705.0   \n",
       "DELAINEY DAVID W                          NaN          3614261.0   \n",
       "\n",
       "                             ...            loan_advances  from_messages  \\\n",
       "TOTAL                        ...               83925000.0            NaN   \n",
       "LAY KENNETH L                ...               81525000.0           36.0   \n",
       "FREVERT MARK A               ...                2000000.0           21.0   \n",
       "BHATNAGAR SANJAY             ...                      NaN           29.0   \n",
       "LAVORATO JOHN J              ...                      NaN         2585.0   \n",
       "SKILLING JEFFREY K           ...                      NaN          108.0   \n",
       "MARTIN AMANDA K              ...                      NaN          230.0   \n",
       "BAXTER JOHN C                ...                      NaN            NaN   \n",
       "BELDEN TIMOTHY N             ...                      NaN          484.0   \n",
       "DELAINEY DAVID W             ...                      NaN         3069.0   \n",
       "\n",
       "                         other  from_this_person_to_poi    poi director_fees  \\\n",
       "TOTAL               42667589.0                      NaN  False     1398517.0   \n",
       "LAY KENNETH L       10359729.0                     16.0   True           NaN   \n",
       "FREVERT MARK A       7427621.0                      6.0  False           NaN   \n",
       "BHATNAGAR SANJAY      137864.0                      1.0  False      137864.0   \n",
       "LAVORATO JOHN J         1552.0                    411.0  False           NaN   \n",
       "SKILLING JEFFREY K     22122.0                     30.0   True           NaN   \n",
       "MARTIN AMANDA K      2818454.0                      0.0  False           NaN   \n",
       "BAXTER JOHN C        2660303.0                      NaN  False           NaN   \n",
       "BELDEN TIMOTHY N      210698.0                    108.0   True           NaN   \n",
       "DELAINEY DAVID W        1661.0                    609.0   True           NaN   \n",
       "\n",
       "                    deferred_income  long_term_incentive  \\\n",
       "TOTAL                   -27992891.0           48521928.0   \n",
       "LAY KENNETH L             -300000.0            3600000.0   \n",
       "FREVERT MARK A           -3367011.0            1617011.0   \n",
       "BHATNAGAR SANJAY                NaN                  NaN   \n",
       "LAVORATO JOHN J                 NaN            2035380.0   \n",
       "SKILLING JEFFREY K              NaN            1920000.0   \n",
       "MARTIN AMANDA K                 NaN            5145434.0   \n",
       "BAXTER JOHN C            -1386055.0            1586055.0   \n",
       "BELDEN TIMOTHY N         -2334434.0                  NaN   \n",
       "DELAINEY DAVID W                NaN            1294981.0   \n",
       "\n",
       "                                 email_address from_poi_to_this_person  \n",
       "TOTAL                                      NaN                     NaN  \n",
       "LAY KENNETH L            kenneth.lay@enron.com                   123.0  \n",
       "FREVERT MARK A          mark.frevert@enron.com                   242.0  \n",
       "BHATNAGAR SANJAY    sanjay.bhatnagar@enron.com                     0.0  \n",
       "LAVORATO JOHN J        john.lavorato@enron.com                   528.0  \n",
       "SKILLING JEFFREY K     jeff.skilling@enron.com                    88.0  \n",
       "MARTIN AMANDA K            a..martin@enron.com                     8.0  \n",
       "BAXTER JOHN C                              NaN                     NaN  \n",
       "BELDEN TIMOTHY N          tim.belden@enron.com                   228.0  \n",
       "DELAINEY DAVID W      david.delainey@enron.com                    66.0  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_df=enron_df.sort_values(by=['total_payments'],ascending=False)\n",
    "enron_df=enron_df.replace('NaN',np.NaN)\n",
    "enron_df[enron_df.total_payments.notnull()].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 POIs and 5 non-POIs and an outlier 'TOTAL' in the Top 10 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>...</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>poi</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>email_address</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOCKHART EUGENE E</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   salary  to_messages  deferral_payments  total_payments  \\\n",
       "LOCKHART EUGENE E     NaN          NaN                NaN             NaN   \n",
       "\n",
       "                   exercised_stock_options  bonus  restricted_stock  \\\n",
       "LOCKHART EUGENE E                      NaN    NaN               NaN   \n",
       "\n",
       "                   shared_receipt_with_poi  restricted_stock_deferred  \\\n",
       "LOCKHART EUGENE E                      NaN                        NaN   \n",
       "\n",
       "                   total_stock_value           ...            loan_advances  \\\n",
       "LOCKHART EUGENE E                NaN           ...                      NaN   \n",
       "\n",
       "                   from_messages  other  from_this_person_to_poi    poi  \\\n",
       "LOCKHART EUGENE E            NaN    NaN                      NaN  False   \n",
       "\n",
       "                  director_fees  deferred_income  long_term_incentive  \\\n",
       "LOCKHART EUGENE E           NaN              NaN                  NaN   \n",
       "\n",
       "                   email_address from_poi_to_this_person  \n",
       "LOCKHART EUGENE E            NaN                     NaN  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron_df[enron_df.drop('poi',axis=1).isnull().all(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one row that does not have any value populated (except for POI ). As this is on the majority category , I would remove as it may not help much. \n",
    "\n",
    "From the initial dataset , I will remove the outlier 'TOTAL' , the row of LOCKHART EUGENE E . Also as the 'email_address' does not help in the prediction , I would drop that column. Also using the helper class featureFormatPandas , I will replace NaN/np.inf with 0 and drop the row that has all NaN / 0 values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_df=enron_df.drop('TOTAL',axis=0)\n",
    "#drop rows that have all NaN values\n",
    "enron_df=enron_df.dropna(thresh=2)\n",
    "enron_df=enron_df.replace('NaN',np.NaN)\n",
    "\n",
    "from feature_format import featureFormatPandas\n",
    "enron_df=featureFormatPandas(enron_df,remove_all_zeroes=True,replace_NaN=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below new features were tried \n",
    "\n",
    "1. total_money_value ,total_poi_interaction  -  Totals of monetary and email attributes , as they should relatively have better significance in identifying a POI than each individual attribute \n",
    "2. mails_to_poi_ratio,mails_from_poi_ratio - ratio of mails from / to poi to the total mails received / sent by an individual - The ratio of total mails to the ones sent / received from POIs may be higher for a POI than others. This attribute is added to explore on those lines. \n",
    "\n",
    "Although none of them had a very huge impact , the ratios and totals definitely helped to improve the scores.\n",
    "\n",
    "I have also explored ratios of deferred income to total payments and deferred stock to total stock - Just to see if POIs (who are aware of the inevitable) would have minimal deferred stock or cash . But these never appeared in the top 10 or 15 and hence have removed them in the final analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_df['total_money_value'] = enron_df['total_payments'] + enron_df['total_stock_value'] \n",
    "enron_df['total_poi_interaction'] = enron_df['shared_receipt_with_poi'] + \\\n",
    "                                    enron_df['from_this_person_to_poi'] + \\\n",
    "                                    enron_df['from_poi_to_this_person'] \n",
    "enron_df['mails_to_poi_ratio']=enron_df['from_this_person_to_poi'].div(enron_df['from_messages'])\n",
    "enron_df['mails_from_poi_ratio']=enron_df['from_poi_to_this_person'].div(enron_df['to_messages'])\n",
    "enron_df=featureFormatPandas(enron_df,remove_all_zeroes=True,replace_NaN=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following phases were employed in the model\n",
    "\n",
    "1. Feature Scaling  \n",
    "2. Feature Selection \n",
    "3. Dimensionality reduction using PCA  \n",
    "4. Various Classifiers were evaluated as this is a classification problem .\n",
    "\n",
    "##### Feature Scaling :\n",
    "\n",
    "As the features are on different scales (no. of mails in a few hundreds to financial features in millions), it is essential for us employ a feature scaling algorithm before passing the data for Classification. In this analysis , I have tried MinMaxScaler.\n",
    "\n",
    "##### Feature Selection : \n",
    "\n",
    "It is essential to choose only relevant and important features and eliminate others 1. To reduce negative impacts of irrelevant features  2. reduce train / test timings .\n",
    "\n",
    "In the analysis, SelectKBest Algorithm with K values varying from 5 to 22 in GridSearchCV was used . From the initial features , 15 features were chosen except email_address,deferral_payments and restricted_stock_deferred . Below are the precision and accuracy scores achieved .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no_new_features</th>\n",
       "      <td>0.72627</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.44106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_poi_interaction</th>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.31255</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.45327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mails_to_poi_ratio</th>\n",
       "      <td>0.7336</td>\n",
       "      <td>0.31381</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.45707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mails_from_poi_ratio</th>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.30341</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.44191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_new_and_old_features</th>\n",
       "      <td>0.73907</td>\n",
       "      <td>0.32232</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.47008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy Precision  Recall  f1score\n",
       "no_new_features           0.72627   0.30303    0.81  0.44106\n",
       "total_poi_interaction      0.7348   0.31255  0.8245  0.45327\n",
       "Mails_to_poi_ratio         0.7336   0.31381   0.841  0.45707\n",
       "mails_from_poi_ratio       0.7262   0.30341   0.813  0.44191\n",
       "all_new_and_old_features  0.73907   0.32232   0.868  0.47008"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "feature_sel_df=pd.DataFrame(columns=['Accuracy','Precision','Recall','f1score'] ,index=['no_new_features','total_poi_interaction','Mails_to_poi_ratio'\n",
    "                                                                            ,'mails_from_poi_ratio','all_new_and_old_features'\n",
    "                                                                           ])\n",
    "\n",
    "feature_sel_df.loc['no_new_features']=pd.Series({'Accuracy':0.72627,'Precision':0.30303,'Recall':0.81000,'f1score':0.44106})\n",
    "feature_sel_df.loc['total_poi_interaction']=pd.Series({'Accuracy':0.73480,'Precision':0.31255,'Recall':0.82450,'f1score':0.45327 })\n",
    "feature_sel_df.loc['Mails_to_poi_ratio']=pd.Series({'Accuracy':0.73360 ,'Precision':0.31381,'Recall':0.84100,'f1score':0.45707})\n",
    "feature_sel_df.loc['mails_from_poi_ratio']=pd.Series({'Accuracy':0.72620,'Precision':0.30341,'Recall':0.81300,'f1score':0.44191})\n",
    "feature_sel_df.loc['all_new_and_old_features']=pd.Series({'Accuracy':0.73907,'Precision':0.32232,'Recall':0.86800,'f1score':0.47008})\n",
    "\n",
    "feature_sel_df\n",
    "\n",
    "#Despite the minimal improvemens , as there are no negative impacts , I have included the new features in the final analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the minimal improvements , as there are no negative impacts , I have included the new features in the final analysis. \n",
    "\n",
    "##### Dimensionality Reduction : \n",
    "\n",
    "Have used PCA to achieve dimensionality reduction. \n",
    "\n",
    "#####Algorithms and Metrics used :\n",
    "\n",
    "The below types of classifiers have been tried \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV,SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier,RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "\n",
    "All the classifiers were evaluated based on their precision , recall and f-score . Accuracy cannot be trusted in this scenario as this is an unbalanced dataset and the classifiers tend to predict everything as non-POI .\n",
    "\n",
    "Precision and Recall performance metrics were used in this analysis to evaluate the model.\n",
    "\n",
    "Precision and recall are defined as : \n",
    "\n",
    "Precision is the ratio of Total True positives (tp) to Total positives predicted (tp + fp) - It is the indication of the how many times the model classified an input as a Positive and how many times was it correct. In our context , low precision indicates that we are classifying a lot of non-POIs as POIs.\n",
    "\n",
    "Recall is the ratio of Total True positives to Total True positives + False Negatives (tp+fn) - It is the indication of how many times the model failed to recognise a positive input as Positive .In our context , low recall means we failed to identify a POI .\n",
    "\n",
    "F1 score  - It indicates a balance between Precision and Recall. It is essential as well as an extremely low precision implies that we will marking a lot of non-POIs as POIs. An extremely high recall means that we are not successfully identifying POIs.\n",
    "\n",
    "##### Algorithm tuning by tuning Parameters (hyperparameters) : \n",
    "\n",
    "*Hyperparameters of an algorithm control the flexibility or freedom of the model , For e.g., the depth or the number of leaves in Decision Trees , C (penalty for misclassification )or the kernel width in SVMs etc., By this, the effects of the Model adopting too much to the training data and the cases of overfitting can be controlled ***\n",
    "\n",
    "All Algorithms provide such parameters which can be tuned to optimize the algorithm and this is almost the final yet very essential step before concluding the model development.\n",
    "\n",
    "Below parameters were primarily considered for tuning .\n",
    "\n",
    "1. C - Value of Regularization Constant - SVM and Linear Model based classifiers - High C Value makes the model complex and tends to overfit. Default value of C=1 did not give any satisfactory performance stats. \n",
    "2. class_weight - This being an unbalanced dataset , class_weight played a crucial role in tuning the algorithms. I have opted for 'balanced' as this uses inverse of the frequencies to calculate the class_weights.\n",
    "3. tol - Training termination parameter - Had to modify the value of this parameter for a few algorithms for better scores.\n",
    "4. selection (k) : Number of features to select in the feature selection algorithms . Have tried small number of features but could not get better scores without using almost all the features. A balance between precision and recall occurred for a k value of 20.\n",
    "5. kernel - This was tuned in the context of SVM (linear , rbf) - Given the less number of samples, rbf performed efficiently\n",
    "6. pca whiten - To reduce the affects due to correlated features , have used whiten==true in PCA .\n",
    "\n",
    "Besides these , have explored different base estimators with AdaBoostClassifier and convicned that SGDClassifier as the base estimator was giving better numbers.\n",
    "\n",
    "##### GridSearchCV : \n",
    "\n",
    "GridSearchCV was used to tune parameters as well as evaluate the estimators all throughout. I have not included all the values explored in the final submission.\n",
    "\n",
    "#####  Cross Validation :\n",
    "\n",
    "Using same data to train and test would lead to over fitting and the model will fail when exposed to unknown data. So it essential that we have separate training and test data.  But having separate testing and training data sets may cause issues if there is lot of variance between the sets. The best possible solution to this problem is to use cross validation. It works by splitting the dataset into k-parts , each split of the data is called a fold. The algorithm is trained on k-1 folds with one held back and tested on the held back fold. This is repeated and the average values of the performance measures are computed.\n",
    "\n",
    "\n",
    "In this model , we have used the StratifiedShuffleSplit from sklearn.cross_validation , which would run for 1000 iterations and on a test data of 10 percent. This is passed to GridSearchCV to be used in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Below is the table of scores for some classifiers \n",
      " Note : Coincidentally LinearSVC and LogisticRegression gave same numbers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VotingClassifier</th>\n",
       "      <td>0.73967</td>\n",
       "      <td>0.32186</td>\n",
       "      <td>0.8605</td>\n",
       "      <td>0.46849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.72947</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>0.45807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.69347</td>\n",
       "      <td>0.29028</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.43886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.69347</td>\n",
       "      <td>0.29028</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.43886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.77993</td>\n",
       "      <td>0.32649</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.42581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeCLassfier</th>\n",
       "      <td>0.62887</td>\n",
       "      <td>0.23745</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>0.36688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Accuracy Precision  Recall  f1score\n",
       "VotingClassifier       0.73967   0.32186  0.8605  0.46849\n",
       "SVC                    0.72947    0.3125  0.8575  0.45807\n",
       "LogisticRegression     0.69347   0.29028   0.899  0.43886\n",
       "LinearSVC              0.69347   0.29028   0.899  0.43886\n",
       "AdaBoostClassifier     0.77993   0.32649   0.612  0.42581\n",
       "DecisionTreeCLassfier  0.62887   0.23745  0.8065  0.36688"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df=pd.DataFrame(columns=['Accuracy','Precision','Recall','f1score'] ,index=['VotingClassifier','SVC','LogisticRegression'\n",
    "                                                                            ,'LinearSVC','AdaBoostClassifier'\n",
    "                                                                           ,'DecisionTreeCLassfier'])\n",
    "\n",
    "scores_df.loc['VotingClassifier']=pd.Series({'Accuracy':0.73967,'Precision':0.32186,'Recall':0.86050,'f1score':0.46849})\n",
    "scores_df.loc['SVC']=pd.Series({'Accuracy':0.72947,'Precision':0.31250,'Recall':0.85750,'f1score':0.45807 })\n",
    "scores_df.loc['LogisticRegression']=pd.Series({'Accuracy':0.69347 ,'Precision':0.29028,'Recall':0.89900,'f1score':0.43886})\n",
    "scores_df.loc['LinearSVC']=pd.Series({'Accuracy':0.69347,'Precision':0.29028,'Recall':0.89900,'f1score':0.43886})\n",
    "scores_df.loc['AdaBoostClassifier']=pd.Series({'Accuracy':0.77993,'Precision':0.32649,'Recall':0.61200,'f1score':0.42581})\n",
    "scores_df.loc['DecisionTreeCLassfier']=pd.Series({'Accuracy':0.62887,'Precision':0.23745,'Recall':0.8065,'f1score':0.36688})\n",
    "\n",
    "print \" Below is the table of scores for some classifiers \"\n",
    "print \" Note : Coincidentally LinearSVC and LogisticRegression gave same numbers\"\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Final Algorithm  chosen : \n",
    "\n",
    "The Voting Classifier from sklearn.ensemble , which combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels . I have used the below classifiers as estimators and passed to a Voting Classifier .\n",
    "\n",
    "1. LogisticRegressionClassifier \n",
    "2. SVC \n",
    "3. AdaBoostClassifier with SGDClassifier as base estimator \n",
    "\n",
    "##### Final Features  included : \n",
    "\n",
    "'salary', 'to_messages', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'total_stock_value', 'expenses', 'loan_advances', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', 'total_poi_interaction', 'mails_to_poi_ratio', 'mails_from_poi_ratio'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6000 out of 6000 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1000 folds for each of 6 candidates, totalling 6000 fits\n",
      "best features selected :\n",
      "['salary', 'to_messages', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'total_stock_value', 'expenses', 'loan_advances', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', 'total_poi_interaction', 'mails_to_poi_ratio', 'mails_from_poi_ratio']\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selection', SelectKBest(k=19, score_func=<function f_classif at 0x00000000096B0588>)), ('pca', PCA(copy=True, n_components=2, whiten=True)), ('classifier', VotingClassifier(estimators=[('lr', LogisticRegression(C=1e-05, class_weight...ning_rate=1.0, n_estimators=1000, random_state=None))],\n",
      "         voting='hard', weights=[1, 1, 1]))])\n",
      "\tAccuracy: 0.73800\tPrecision: 0.32077\tRecall: 0.86350\tF1: 0.46777\tF2: 0.64517\n",
      "\tTotal predictions: 15000\tTrue positives: 1727\tFalse positives: 3657\tFalse negatives:  273\tTrue negatives: 9343\n",
      "\n",
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :     281.43 s.\n",
      "  System :       0.00 s.\n",
      "Wall time:     281.44 s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aswani\\Anaconda\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [11] are constant.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "run -t poi_id.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####References : \n",
    "1. Intro to Machine Learning Udacity course \n",
    "2. Sklearn Documentation \n",
    "3. Few topics on MachineLearningMastery blog by Jason Brownlee : http://machinelearningmastery.com/ \n",
    "4. Python for Data Analysis book by Wes McKinney\n",
    "5. Sebastian Raschka blog and book (on safari books)\n",
    "6. Lot of stackexchange and stackoverflow discussions \n",
    "7. Wikipaedia for few definitions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
